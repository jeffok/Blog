---
title: ceph小优化
date: 2022-01-30 17:45:00
tags: ["存储", "Ceph"]
categories: ["Ceph"]
render_with_liquid: false
permalink: /posts/2022-01-30-ceph小优化/
---

本文档记录 Ceph 相关的性能优化配置和命令。

## KVM 虚拟机磁盘 IO 限制

使用 `virsh blkdeviotune` 命令限制虚拟机的磁盘 IO：

```bash
virsh blkdeviotune instance-00000139 sda --read-iops-sec 40000 --write-iops-sec 40000 --config --live

virsh blkdeviotune instance-00000136 sda --read-iops-sec 60000 --write-iops-sec 60000 --read-bytes-sec 419430400 --write-bytes-sec 419430400 --config --live
virsh blkdeviotune instance-00000136 sdb --read-iops-sec 60000 --write-iops-sec 60000 --read-bytes-sec 419430400 --write-bytes-sec 419430400 --config --live

virsh blkdeviotune instance-00000139 sda --read-iops-sec 60000 --write-iops-sec 60000 --read-bytes-sec 419430400 --write-bytes-sec 419430400 --config --live
virsh blkdeviotune instance-00000139 sdb --read-iops-sec 60000 --write-iops-sec 60000 --read-bytes-sec 419430400 --write-bytes-sec 419430400 --config --live

virsh blkdeviotune instance-00000136 sda --read-bytes-sec 629145600 --write-bytes-sec 419430400  --read-iops-sec 40000 --write-iops-sec 40000  --read-bytes-sec-max 629145600 --write-bytes-sec-max 419430400  --group-name drive-scsi0-0-0-1 --read-bytes-sec-max-length 1 --write-bytes-sec-max-length 1 --read-iops-sec-max-length 1 --write-iops-sec-max-length 1

virsh blkdeviotune instance-00000136 sdb --read-bytes-sec 629145600 --write-bytes-sec 419430400  --read-iops-sec 40000 --write-iops-sec 40000  --read-bytes-sec-max 629145600 --write-bytes-sec-max 419430400  --group-name drive-scsi0-0-0-1 --read-bytes-sec-max-length 1 --write-bytes-sec-max-length 1 --read-iops-sec-max-length 1 --write-iops-sec-max-length 1

6145e076-b707-48da-9c4b-74677a0432ac

3cd62cee-aab0-4045-824d-b8be5513879c

ceph tell osd.* injectargs --rbd_cache_enabled=true

ceph tell osd.* injectargs --rbd_op_threads=16

ceph tell osd.* injectargs --filestore_op_threads=16
ceph tell osd.* injectargs --rbd_cache_size=67108864
ceph tell osd.* injectargs --rbd_cache_max_dirty=50331648
ceph tell osd.* injectargs --rbd_cache_target_dirty=33554432
ceph tell osd.* injectargs --rbd_cache_max_dirty_age=5
ceph tell osd.* injectargs --rbd_cache_writethrough_until_flush=flase
ceph tell osd.* injectargs --osd_op_num_shards=8

ceph tell osd.* injectargs --objecter_inflight_ops=819200

对于顺序读的场景，调大read_ahead_kb的值有时可以提升顺序读的性能。

## Bluestore 性能优化

### Bluestore Cache

首先是 Bluestore cache，因为 bluestore 没有用文件系统，只是因为 rocksdb 而实现了一个轻量级的 bluefs，所以得自己实现 cache 功能，就像文件系统中的 pagecache。bluestore cache 对性能影响很大，考虑到当前系统的内存一般都很大，bluestore cache 默认是 4G，我们建议在系统允许范围内尽量地调大 bluestore cache，以得到最佳的性能。

### Bluestore Cache Autotuning

自从 Ceph Luminous 版本出来后，Ceph 引入了一个新的特性，叫做 bluestore cache autotuning，这个特性默认是启用的，主要功能是根据当前的系统负载来动态调整 cache 的分配比例。在大多数情况下，这个特性能使当前应用场景性能达到最佳，但在一些特殊的场景中，如果关闭这个特性，然后手工的来根据当前的特点来调整 cache 的分配比例，可以达到更好的性能。

### RocksDB 优化

最后考虑到 rocksdb 在 bluestore 中的重要性，`bluestore_rocksdb_options` 这个参数的调整有时可以极大地提高整个集群的性能。

## 不同场景的性能瓶颈分析

- **对于 4k Seq Write 用例**：从 sar log 分析，热点是 SSD 磁盘，考虑到 SSD 用来作为 rocksdb 的 block db，所以可以从 rocksdb 层面来调优。
- **对于 4k Rand Write 用例**：从 sar log 分析，HDD 的磁盘使用率接近 100%，所以 HDD 是瓶颈，可以考虑从 I/O scheduler 层面来调优。
- **对于 4M Seq/Rand Write 用例**：明显可以看出，网络带宽已经到了瓶颈，这个可以从网络层面来调优。

## RocksDB 配置示例

```ini
bluestore_rocksdb_options = compression=kNoCompression,max_write_buffer_number=64,min_write_buffer_number_to_merge=32,recycle_log_file_num=64,compaction_style=kCompactionStyleLevel,write_buffer_size=16M,target_file_size_base=4MB,max_background_compactions=64,level0_file_num_compaction_trigger=64,level0_slowdown_writes_trigger=128,level0_stop_writes_trigger=256,max_bytes_for_level_base=6GB,compaction_threads=32,flusher_threads=8,compaction_readahead_size=2MB

```