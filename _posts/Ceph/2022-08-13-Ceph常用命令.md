---
title: Ceph常用命令
date: 2022-08-13 08:00:00
tags: ["存储", "Ceph", "RBD"]
categories: ["Ceph"]
render_with_liquid: false
permalink: /posts/2022-08-13-Ceph常用命令/
---

本文档整理了 Ceph 集群运维中的常用命令。

## 参考资源

- [ceph 常用命令](https://www.cnblogs.com/zhongguiyao/p/7992729.html)
- [常用表格汇总](http://www.xuxiaopang.com/2016/11/11/doc-ceph-table/)
- [PG 计算器](https://ceph.com/pgcalc/)
- [很牛的东西](http://www.51niux.com/?id=164)

##### ceph osd 进行不能正常启动或启动失败可以尝试以下命令

```bash
# 重置服务

systemctl reset-failed ceph-osd@45.service

# 手动挂载

mount /dev/sdh1 /var/lib/ceph/osd/ceph-45

```

##### 查询当前磁盘挂载的位置

```bash
rbd status <rbd-name> -p <pool-name>

```

##### 调整PG数

```sql
ceph osd pool set rbd pg_num 256
ceph osd pool set rbd pgp_num 256

```

##### raw格式导入到rbd

```bash
rbd import --image-format 2 --dest-pool rbd-vms --dest dest_rbd c7image.raw

```

##### rbd格式导出为raw格式

```bash
rbd export rbd-images/c546c0f9-ad57-42a0-ad38-ec54a35e3300 centos7.5_init_20180521.raw

```

##### 如果未删除干净可能会存在添加OSD失败

```bash
ceph auth list  # 查看osd key
ceph auth del osd.xxx  # 删除失败的key

```

##### 查看ceph当前运行的参数

```sql
ceph daemon osd.0 config show

```

##### 查看osd_down后多久平衡数据

```bash
# osd down
# mon_osd_down_out_interval

ceph daemon osd.0 config show | grep mon_osd_down_out_interval

# 默认为300s

```

##### 设置osd不进行数据平衡

```sql
ceph osd set noout

```

##### 调整reweight值

```bash
ceph osd reweight 236 1.0

```

##### 查看pool pg值

```bash
ceph osd pool get rbd-vms pg_num  # rbd-vms 为pool的名称

```

##### 查看虚拟机数量、卷数量

```bash
rbd -p rbd-vms ls | wc -l
rbd -p rbd-volume ls | wc -l  # 查看pool使用数量

```

##### ceph物理机down机后，平衡数据时限制IO性能

```ini
osd_recovery_sleep =0.5(默认0)
osd_recovery_op_priority=10(默认3)

```

##### 暂停ceph集群数据平衡

```sql
ceph osd set nobackfill
ceph osd set norecover
ceph osd set norebalance

```

##### 恢复ceph集群数据平衡

```sql
ceph osd unset nobackfill
ceph osd unset norecover
ceph osd unset norebalance

```

##### 设置ceph参数立即生效

```bash

# 修改ceph平衡IO

ceph tell osd.* injectargs --osd_op_thread_suicide_timeout 120
ceph tell osd.* injectargs --osd_max_backfills 1
ceph tell osd.* injectargs --osd_recovery_max_active 1

```

##### 设置磁盘权重

```bash
ceph osd crush reweight osd.x 0.2  # 立即生效

```

##### 调整rack

```bash

# 添加一个host node-1

ceph osd crush add-bucket node-1 host
ceph osd crush add-bucket bjdz_cloud_ceph17_a0914-02_32117 host

# 移动到root下面

ceph osd crush move node-1 root=default
ceph osd crush move bjdz-rack-09 root=bjdz-ceph
ceph osd crush move bjdz_cloud_ceph17_a0914-02_32117 rack=bjdz-rack-09 root=bjdz-ceph

# 添加osd到此host下

ceph osd crush add osd.16 0.01900 root=default host=node-1
ceph osd crush add $i 0.2 root=bjdz-ceph rack=bjdz-rack-09 host=bjdz_cloud_ceph17_a0914-02_32117

# 删除host

ceph osd crush remove $host

```

参考：[CRUSH Map 文档](https://docs.ceph.com/docs/mimic/rados/operations/crush-map/)

##### 查看ceph存储空间

```bash
ceph df

```

##### 删除一个节点的所有的ceph数据包

```bash
ceph-deploy purge node1
ceph-deploy purgedata node1

```

##### 为ceph创建一个admin用户并为admin用户创建一个密钥，把密钥保存到/etc/ceph目录下：

```sql
ceph auth get-or-create client.admin mds 'allow' osd 'allow *' mon 'allow *' > /etc/ceph/ceph.client.admin.keyring
或
ceph auth get-or-create client.admin mds 'allow' osd 'allow *' mon 'allow *' -o /etc/ceph/ceph.client.admin.keyring

```

##### 为osd.0创建一个用户并创建一个key

```sql
ceph auth get-or-create osd.0 mon 'allow rwx' osd 'allow *' -o /var/lib/ceph/osd/ceph-0/keyring

```

##### 为mds.node1创建一个用户并创建一个key

```sql
ceph auth get-or-create mds.node1 mon 'allow rwx' osd 'allow *' mds 'allow *' -o /var/lib/ceph/mds/ceph-node1/keyring

```

##### 查看ceph集群中的认证用户及相关的key

```bash
ceph auth list

```

##### 删除集群中的一个认证用户

```bash
ceph auth del osd.0

```

##### 查看集群的详细配置

```sql
ceph daemon mon.node1 config show | more

```

##### 查看集群健康状态细节

```bash
ceph health detail

```

##### 查看ceph log日志所在的目录

```sql
ceph-conf --name mon.node1 --show-config-value log_file

```

##### 查看mon的状态信息

```bash
ceph mon stat

```

##### 查看mon的选举状态

```bash
ceph quorum_status

```

##### 查看mon的映射信息

```bash
ceph mon dump

```

##### 删除一个mon节点

```bash
ceph mon remove node1

```

##### 获得一个正在运行的mon map，并保存在1.txt文件中

```bash
ceph mon getmap -o 1.txt

# 查看上面获得的map

monmaptool --print 1.txt

```

##### 把上面的mon map注入新加入的节点

```bash
ceph-mon -i node4 --inject-monmap 1.txt

```

##### 查看mon的amin socket

```sql
ceph-conf --name mon.node1 --show-config-value admin_socket

```

##### 查看mon的详细状态

```bash
ceph daemon mon.node1  mon_status

```

##### 查看msd状态

```bash
ceph mds stat

```

##### 查看msd的映射信息

```bash
ceph mds dump

```

##### 删除一个mds节点

```bash
ceph  mds rm 0 mds.node1

```

##### 查看ceph osd运行状态

```bash
ceph osd stat

```

##### 查看osd映射信息

```bash
ceph osd dump

```

##### 查看osd的目录树

```bash
ceph osd tree

```

##### down掉一个osd硬盘

```bash
ceph osd down 0   #down掉osd.0硬盘

```

##### 在集群中删除一个osd硬盘

```bash
ceph osd rm 0

```

##### 在集群中删除一个osd 硬盘 crush map

```bash
ceph osd crush rm osd.0

```

##### 在集群中删除一个osd的host节点

```bash
ceph osd crush rm node1

```

##### 查看最大osd的个数

```bash
ceph osd getmaxosd

# max_osd = 4 in epoch 514           #默认最大是4个osd节点

```

##### 设置最大的osd的个数（当扩大osd节点的时候必须扩大这个值）

```bash
ceph osd setmaxosd 10

```

##### 设置osd crush的权重为1.0

```sql
ceph osd crush set {id} {weight} [{loc1} [{loc2} ...]]

# 例----

ceph osd crush set 3 3.0 host=node4
set item id 3 name 'osd.3' weight 3 at location {host=node4} to crush map

# 或

ceph osd crush reweight osd.3 1.0

```

##### 设置osd的权重

```bash
ceph osd reweight 3 0.5

```

##### 把一个osd节点逐出集群

```bash
ceph osd out osd.3

```

##### 把逐出的osd加入集群

```bash
ceph osd in osd.3

```

##### 暂停osd （暂停后整个集群不再接收数据）

```bash
ceph osd pause

```

##### 再次开启osd （开启后再次接收数据）

```bash
ceph osd unpause

```

##### 查看一个集群osd.2参数的配置

```sql
ceph --admin-daemon /var/run/ceph/ceph-osd.2.asok config show | less

```

### PG

##### 查看pg组的映射信息

```bash
ceph pg dump

```

##### 查看一个PG的map

```bash
ceph pg map 0.3f

```

##### 查看PG状态

```bash
ceph pg stat

```

##### 查询一个pg的详细信息

```bash
ceph pg  0.26 query

```

##### 查看pg中stuck的状态

```bash
ceph pg dump_stuck unclean
ceph pg dump_stuck inactive
ceph pg dump_stuck stale

```

##### 显示一个集群中的所有的pg统计

```bash
ceph pg dump --format plain

```

##### 恢复一个丢失的pg

```bash
ceph pg {pg-id} mark_unfound_lost revert

```

##### 显示非正常状态的pg

```bash
ceph pg dump_stuck inactive|unclean|stale

```

### Pool

##### 查看ceph集群中的pool数量

```bash
ceph osd lspools

```

##### 在ceph集群中创建一个pool

```sql
ceph osd pool create jiayuan 100            #这里的100指的是PG组

```

##### 为一个ceph pool配置配额

```bash
ceph osd pool set-quota data max_objects 10000

```

##### 在集群中删除一个pool

```sql
ceph osd pool delete jiayuan  jiayuan  --yes-i-really-really-mean-it  #集群名字需要重复两次

```

##### 显示集群中pool的详细信息

```bash
rados df

```

##### 给一个pool创建一个快照

```bash
ceph osd pool mksnap data   date-snap

```

##### 删除pool的快照

```bash
ceph osd pool rmsnap data date-snap

```

##### 查看data池的pg数量

```bash
ceph osd pool get data pg_num

```

##### 设置data池的最大存储空间为100T（默认是1T)

```sql
ceph osd pool set data target_max_bytes 100000000000000

# set pool 0 target_max_bytes to 100000000000000

```

##### 设置data池的副本数是3

```sql
ceph osd pool set data size 3

```

##### 设置data池能接受写操作的最小副本为2

```sql
ceph osd pool set data min_size 2

```

##### 查看集群中所有pool的副本尺寸

```bash
ceph osd dump | grep 'replicated size'

```

##### 设置一个pool的pg数量

```sql
ceph osd pool set data pg_num 100

```

##### 设置一个pool的pgp数量

```sql
ceph osd pool set data pgp_num 100

```

### rados和rbd指令

##### 查看ceph集群中有多少个pool （只是查看pool)

```bash
rados lspools

```

##### 查看ceph集群中有多少个pool,并且每个pool容量及利用情况

```bash
rados df

```

##### 创建一个pool

```bash
rados mkpool test

```

##### 查看ceph pool中的ceph object （这里的object是以块形式存储的）

```bash
rados ls -p volumes | more

```

##### 创建一个对象object

```sql
rados create test-object -p test
rados -p test ls  # 查看

```

##### 删除一个对象

```bash
rados rm test-object-1 -p test

```

##### 查看ceph中一个pool里的所有镜像

```bash
rbd ls images

```

##### 查看ceph pool中一个镜像的信息

```bash
rbd info -p images --image 74cb427c-cee9-47d0-b467-af217a67e60a

```

##### 在test池中创建一个命名为zhanguo的10000M的镜像

```sql
rbd create -p test --size 10000 zhanguo
rbd -p test info zhanguo    #查看新建的镜像的信息

```

##### 删除一个镜像

```bash
rbd rm  -p test  lizhanguo

```

##### 调整一个镜像的尺寸

```bash
rbd resize -p test --size 20000 zhanguo
rbd -p test info zhanguo   #调整后的镜像大小

```

##### 给一个镜像创建一个快照

```sql
rbd  snap create  test/zhanguo@zhanguo123  #池/镜像@快照
rbd   snap ls  -p test zhanguo
rbd info test/zhanguo@zhanguo123

```

##### 查看一个镜像文件的快照

```bash
rbd snap ls  -p volumes volume-7687988d-16ef-4814-8a2c-3fbd85e928e4

```

##### 删除一个镜像文件的一个快照快照

```bash
rbd snap rm volumes/volume-7687988d-16ef-4814-8a2c-3fbd85e928e4@snapshot-ee7862aa-825e-4004-9587-879d60430a12

# rbd: snapshot 'snapshot-60586eba-b0be-4885-81ab-010757e50efb' is protected from removal.

# 2014-08-18 19:23:42.099301 7fd0245ef760 -1 librbd: removing snapshot from header failed: (16) Device or resource busy

# 上面不能删除显示的报错信息是此快照备写保护了，下面命令是删除写保护后再进行删除。

rbd snap unprotect volumes/volume-7687988d-16ef-4814-8a2c-3fbd85e928e4@snapshot-ee7862aa-825e-4004-9587-879d60430a12
rbd snap rm volumes/volume-7687988d-16ef-4814-8a2c-3fbd85e928e4@snapshot-ee7862aa-825e-4004-9587-879d60430a12

```

##### 删除一个镜像文件的所有快照

```bash
rbd snap purge  -p volumes volume-7687988d-16ef-4814-8a2c-3fbd85e928e4

```

##### 导出镜像

```bash
rbd export -p images --image 74cb427c-cee9-47d0-b467-af217a67e60a /root/aaa.img

```

##### 导出云硬盘

```bash
rbd export -p volumes --image volume-470fee37-b950-4eef-a595-d7def334a5d6 /var/lib/glance/ceph-pool/volumes/Message-JiaoBenJi-10.40.212.24

```

##### 把一个镜像导入ceph中 （但是直接导入是不能用的，因为没有经过openstack,openstack是看不到的）

```bash
rbd import /root/aaa.img -p images --image 74cb427c-cee9-47d0-b467-af217a67e60a

```
